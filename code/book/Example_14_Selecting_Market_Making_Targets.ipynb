{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 14 - Selecting Market Making Targets\n",
    "\n",
    "In this example, we'll filter for good market making targets according to the following criteria:\n",
    "\n",
    "1. **Liquidity** - the extent to which assets can be bought or sold at stable prices \\(typically reflected by volume\\).\n",
    "2. **Profitable Spread Ratio** - whether the spread ratio \\(ask / bid\\) is usually profitable for an asset.\n",
    "3. **Return** - the return on the funds we use for market making.\n",
    "4. **Balanced Volume** - we'll only consider assets with reasonable volume on both sides of the book.\n",
    "5. **Competition** - we'll limit ourselves to assets without too much competition.\n",
    "\n",
    "As discussed in the text, we'll restrict our analysis to one day of the week \\(Saturday\\) over several months of history.\n",
    "\n",
    "Note that we've already developed all of the tools we need to perform our analysis from previous chapters. This example, therefore, largely consists of choosing thresholds for existing tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame, Series\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "# EveKit imports\n",
    "from evekit.reference import Client\n",
    "from evekit.util import convert_raw_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using region_id=10000002, station_id=60003760 from 2017-01-07 00:00:00 to 2017-05-20 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# As in previous examples, we'll consider opportunities in The Forge at the busiest station in Jita.\n",
    "sde_client = Client.SDE.get()\n",
    "region_query = \"{values: ['The Forge']}\"\n",
    "station_query = \"{values: ['Jita IV - Moon 4 - Caldari Navy Assembly Plant']}\"\n",
    "region_id = sde_client.Map.getRegions(regionName=region_query).result()[0][0]['regionID']\n",
    "station_id = sde_client.Station.getStations(stationName=station_query).result()[0][0]['stationID']\n",
    "date_range = pd.date_range(datetime.datetime(2017, 1, 7), datetime.datetime(2017, 5, 20))\n",
    "print(\"Using region_id=%d, station_id=%d from %s to %s\" % (region_id, station_id, str(date_range[0]), str(date_range[-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11781"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll filter for liquidity first, starting from all available types.\n",
    "#\n",
    "market_types = Client.SDE.load_complete(sde_client.Inventory.getTypes, marketGroupID=\"{start: 0, end: 1000000000}\")\n",
    "market_type_map = {}\n",
    "for x in market_types:\n",
    "    market_type_map[x['typeID']] = x\n",
    "len(market_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving 2017-01-07 00:00:00...done\n",
      "Retrieving 2017-01-14 00:00:00...done\n",
      "Retrieving 2017-01-21 00:00:00...done\n",
      "Retrieving 2017-01-28 00:00:00...done\n",
      "Retrieving 2017-02-04 00:00:00...done\n",
      "Retrieving 2017-02-11 00:00:00...done\n",
      "Retrieving 2017-02-18 00:00:00...done\n",
      "Retrieving 2017-02-25 00:00:00...done\n",
      "Retrieving 2017-03-04 00:00:00...done\n",
      "Retrieving 2017-03-11 00:00:00...done\n",
      "Retrieving 2017-03-18 00:00:00...done\n",
      "Retrieving 2017-03-25 00:00:00...done\n",
      "Retrieving 2017-04-01 00:00:00...done\n",
      "Retrieving 2017-04-08 00:00:00...done\n",
      "Retrieving 2017-04-15 00:00:00...done\n",
      "Retrieving 2017-04-22 00:00:00...done\n",
      "Retrieving 2017-04-29 00:00:00...done\n",
      "Retrieving 2017-05-06 00:00:00...done\n",
      "Retrieving 2017-05-13 00:00:00...done\n",
      "Retrieving 2017-05-20 00:00:00...done\n"
     ]
    }
   ],
   "source": [
    "# We only want to consider Saturdays so we'll construct a revised date range and load market\n",
    "# history for just those days.\n",
    "#\n",
    "from evekit.marketdata import MarketHistory\n",
    "sat_date_range = [x for x in date_range if x.weekday() == 5]\n",
    "market_history = MarketHistory.get_data_frame(dates=sat_date_range, types=market_type_map.keys(), regions=[region_id], \n",
    "                                              config=dict(local_storage=\".\", tree=True, skip_missing=True, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Include the liquidity filter framework from previous examples.\n",
    "#\n",
    "def liquid_types(history, liquidp, verbose=False):\n",
    "    # Result is a map from region to set of liquid types for that region\n",
    "    # Iterate through all types contained in the history object\n",
    "    liquid_map = {}\n",
    "    count = 0\n",
    "    # Iterate through all regions and types\n",
    "    for next_region in history.region_id.unique():\n",
    "        liquid_set = set()\n",
    "        by_region = history[history.region_id == next_region]\n",
    "        for next_type in by_region.type_id.unique():\n",
    "            by_type = by_region[by_region.type_id == next_type]\n",
    "            if liquidp(next_region, next_type, by_type):\n",
    "                liquid_set.add(next_type)\n",
    "            count += 1\n",
    "            if count % 1000 == 0 and verbose:\n",
    "                print(\"Tested %d (region, type) pairs\" % count)\n",
    "        liquid_map[next_region] = liquid_set\n",
    "    return liquid_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In this example, we'll filter for assets which trade almost every\n",
    "# day in our historic range, and which have a reasonable number\n",
    "# of orders (NOT volume!).\n",
    "#\n",
    "def liquidity_filter(min_days, min_count):\n",
    "    def liquidp(region_id, type_id, history):\n",
    "        return len(history) >= min_days and \\\n",
    "               len(history[history.order_count < min_count]) == 0\n",
    "    return liquidp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We'll use the following values to parameterize the liquidity filter\n",
    "#\n",
    "# Minimum number of orders per day\n",
    "min_count = 500\n",
    "# Each type we consider must trade every day\n",
    "min_values = len(market_history.index.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now compute liquid types.  The liquidity filter returns a map from region to the\n",
    "# set of liquid types in that region.\n",
    "#\n",
    "liquid_type_map = liquid_types(market_history, liquidity_filter(min_values, min_count))\n",
    "len(liquid_type_map[region_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to eliminate any types which do not have profitable spreads for the majority of each day of trading.  Recall from the text that a spread is profitable if:\n",
    "\n",
    "${p_a \\over p_b} > {{1 + b}\\over{1 - t - b}}$\n",
    "\n",
    "where:\n",
    "\n",
    "* $p_a$ - is the best ask\n",
    "* $p_b$ - is the best bid\n",
    "* $b$ - is the broker fee rate for posting a limit order\n",
    "* $t$ - is the sales tax rate for filling a buy order\n",
    "\n",
    "Moreover, we want to eliminate any types which do not meet a given return target for the majority of each trading day.  Since a type which meets a certain return target must also be profitable, we can combine these two filters into a single test.  From the text \\(and using the same definitions as above\\), we know that return is given by the equation:\n",
    "\n",
    "${{p_a}\\over{p_a \\times t + b \\times (p_a + p_b) + p_b}} - 1$\n",
    "\n",
    "Since return may be different for each order book snapshot in a given day, we'll compute the median of the snapshot returns and use that value as our measured return for the day.  We'll retain the assets for which this measured return exceeds the target return for at least half the days in our trading range.  This will give us a good selection of asset types which are often profitable.  We'll set an initial return target of 10%.\n",
    "\n",
    "To compute return, we'll also need values for sales tax rate and broker fee rate.  We'll set these to 1% and 2.5%, respectively, which are typical values at NPC stations with max skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving 2017-01-07 00:00:00...done\n",
      "Retrieving 2017-01-14 00:00:00...done\n",
      "Retrieving 2017-01-21 00:00:00...done\n",
      "Retrieving 2017-01-28 00:00:00...done\n",
      "Retrieving 2017-02-04 00:00:00...done\n",
      "Retrieving 2017-02-11 00:00:00...done\n",
      "Retrieving 2017-02-18 00:00:00...done\n",
      "Retrieving 2017-02-25 00:00:00...done\n",
      "Retrieving 2017-03-04 00:00:00...done\n",
      "Retrieving 2017-03-11 00:00:00...done\n",
      "Retrieving 2017-03-18 00:00:00...done\n",
      "Retrieving 2017-03-25 00:00:00...done\n",
      "Retrieving 2017-04-01 00:00:00...done\n",
      "Retrieving 2017-04-08 00:00:00...done\n",
      "Retrieving 2017-04-15 00:00:00...done\n",
      "Retrieving 2017-04-22 00:00:00...done\n",
      "Retrieving 2017-04-29 00:00:00...done\n",
      "Retrieving 2017-05-06 00:00:00...done\n",
      "Retrieving 2017-05-13 00:00:00...done\n",
      "Retrieving 2017-05-20 00:00:00...done\n"
     ]
    }
   ],
   "source": [
    "# We normally won't be able to load the entire date range unless the type set is very small.  So for this\n",
    "# example, we iterate through the date set and build a dataframe giving the daily median return for each \n",
    "# type.\n",
    "#\n",
    "from evekit.marketdata import OrderBook\n",
    "#\n",
    "broker_rate = 0.025\n",
    "sales_tax_rate = 0.01\n",
    "#\n",
    "targets = liquid_type_map[region_id]\n",
    "spread_data = []\n",
    "for next_date in sat_date_range:\n",
    "    order_book = OrderBook.get_data_frame(dates=[next_date], types=targets, regions=[region_id], \n",
    "                                          config=dict(local_storage=\".\", tree=True, skip_missing=True, \n",
    "                                                      fill_gaps=True, verbose=True))\n",
    "    # Compute best bid and ask for each snapshot and type\n",
    "    by_loc = order_book[order_book.location_id == station_id]\n",
    "    bids = by_loc[by_loc.buy == True]\n",
    "    asks = by_loc[by_loc.buy == False]\n",
    "    bids_grouped = bids.groupby([bids.index, bids.type_id]).price.max()\n",
    "    asks_grouped = asks.groupby([asks.index, asks.type_id]).price.min()\n",
    "    bids_us = bids_grouped.unstack()\n",
    "    asks_us = asks_grouped.unstack()\n",
    "    #\n",
    "    # Now compute the median ask/bid ratio for each type\n",
    "    for type_id in targets:\n",
    "        joined = pd.concat([asks_us.xs(type_id, axis=1).rename(\"ask\"), bids_us.xs(type_id, axis=1).rename(\"bid\")], axis=1)\n",
    "        returns = (joined.ask / (joined.ask * sales_tax_rate + broker_rate * (joined.ask + joined.bid) + joined.bid)) - 1\n",
    "        median = returns.median()\n",
    "        spread_data.append(dict(day=next_date, type_id=type_id, median_return=median))\n",
    "#\n",
    "# Finally, we convert the spread ratio data into a dataframe\n",
    "spread_df = pd.DataFrame(spread_data, index=[x['day'] for x in spread_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now filter down to the assets which have a daily median return which exceeds our target\n",
    "# for every day in our date range.  Obviously, you can increase the number of assets which pass\n",
    "# the filter by lowering the return target.\n",
    "#\n",
    "return_target = 0.1\n",
    "high_return = spread_df[spread_df.median_return > return_target]\n",
    "targets = high_return.type_id.groupby(high_return.type_id).count()\n",
    "targets = list(targets[targets >= len(sat_date_range)/2].index)\n",
    "len(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Market making requires both buying and selling the same asset type several times a day.  Therefore, the best assets for market making will have reasonable volume on both sides of the book.  Our next filter will infer trades for each of our assets over the given date range, then compute buy and sell trade volume.  If both volumes exceed a reasonable threshold, then we know there is likely enough trade volume on both sides of the book to support market making.\n",
    "\n",
    "Since we need to infer trades, we'll first set up our trade inference code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving 2016-12-10 00:00:00...done\n",
      "Retrieving 2016-12-17 00:00:00...done\n",
      "Retrieving 2016-12-24 00:00:00...done\n",
      "Retrieving 2016-12-31 00:00:00...done\n",
      "Retrieving 2017-01-07 00:00:00...done\n",
      "Retrieving 2017-01-14 00:00:00...done\n",
      "Retrieving 2017-01-21 00:00:00...done\n",
      "Retrieving 2017-01-28 00:00:00...done\n",
      "Retrieving 2017-02-04 00:00:00...done\n",
      "Retrieving 2017-02-11 00:00:00...done\n",
      "Retrieving 2017-02-18 00:00:00...done\n",
      "Retrieving 2017-02-25 00:00:00...done\n",
      "Retrieving 2017-03-04 00:00:00...done\n",
      "Retrieving 2017-03-11 00:00:00...done\n",
      "Retrieving 2017-03-18 00:00:00...done\n",
      "Retrieving 2017-03-25 00:00:00...done\n",
      "Retrieving 2017-04-01 00:00:00...done\n",
      "Retrieving 2017-04-08 00:00:00...done\n",
      "Retrieving 2017-04-15 00:00:00...done\n",
      "Retrieving 2017-04-22 00:00:00...done\n",
      "Retrieving 2017-04-29 00:00:00...done\n",
      "Retrieving 2017-05-06 00:00:00...done\n",
      "Retrieving 2017-05-13 00:00:00...done\n",
      "Retrieving 2017-05-20 00:00:00...done\n"
     ]
    }
   ],
   "source": [
    "# In preparation for trade inference, we need to commpute a volume threshold series for each type\n",
    "# which will help us distinguish between large orders and cancels.  Note that our threshold computer\n",
    "# requires a window of five days which means we're missing four weekends prior to our start date.\n",
    "# To fix this, we'll reload market history here.\n",
    "#\n",
    "ext_date_range = [datetime.datetime(2016, 12, 10), datetime.datetime(2016, 12, 17),\n",
    "                  datetime.datetime(2016, 12, 24), datetime.datetime(2016, 12, 31)] + sat_date_range\n",
    "ext_market_history = MarketHistory.get_data_frame(dates=ext_date_range, types=targets, regions=[region_id], \n",
    "                                                  config=dict(local_storage=\".\", tree=True, skip_missing=True, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we can compute the thresholds we need for trade inferrence\n",
    "#\n",
    "volume_thresh_multiplier = 0.04\n",
    "volume_thresh_map = {}\n",
    "for next_type in ext_market_history.groupby(ext_market_history.type_id):\n",
    "    group_id = next_type[0]\n",
    "    group_df = next_type[1]\n",
    "    volume_thresh_map[group_id] = group_df.volume.rolling(window=5, center=False).mean() * volume_thresh_multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For our trade side filter, it suffices to just capture the per side volume.  This allows for a simplified \n",
    "# trade inference function.\n",
    "#\n",
    "# Compute the buy and sell side trade volume for the given type on the given date\n",
    "# using the given order book.  Large trades and cancels are distinguished using\n",
    "# the given volume threshold map.\n",
    "#\n",
    "def compute_side_volume(type_id, dt, order_book, volume_threshold_map):\n",
    "    buy_volume = 0\n",
    "    sell_volume = 0\n",
    "    by_type = order_book[order_book.type_id == type_id]\n",
    "    vol_limit = volume_threshold_map[type_id][dt]\n",
    "    #\n",
    "    # Iterate over consecutive book snapshots looking for order book changes.\n",
    "    #\n",
    "    snap_list = list(by_type.groupby(by_type.index))\n",
    "    snap_pairs = zip(snap_list, snap_list[1:])\n",
    "    for current, next in snap_pairs:\n",
    "        current_snap = current[1]\n",
    "        current_time = current[0]\n",
    "        next_snap = next[1]\n",
    "        next_time = next[0]\n",
    "        # Look for volume changes.  These are trades.\n",
    "        merged = pd.merge(current_snap, next_snap, on=\"order_id\")\n",
    "        changed_orders = merged[merged.volume_x != merged.volume_y]\n",
    "        for next_change in changed_orders.index:\n",
    "            # Create the trade object\n",
    "            next_line = changed_orders.ix[next_change]\n",
    "            amount = next_line.volume_x - next_line.volume_y \n",
    "            if next_line.buy_x:\n",
    "                buy_volume += amount\n",
    "            else:\n",
    "                sell_volume += amount\n",
    "        # Look for removed orders.  These are either a fully filled order or a cancel.\n",
    "        removed_orders = set(current_snap.order_id).difference(set(next_snap.order_id))\n",
    "        for order_id in removed_orders:\n",
    "            next_line = current_snap[current_snap.order_id == order_id].ix[current[0]]\n",
    "            # If the volume of a removed order does not exceed the threshold, then it's a trade\n",
    "            if next_line.volume <= vol_limit:\n",
    "                if next_line.buy:\n",
    "                    buy_volume += next_line.volume\n",
    "                else:\n",
    "                    sell_volume += next_line.volume\n",
    "    # Return result\n",
    "    return (buy_volume, sell_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving 2017-01-07 00:00:00...done\n",
      "Retrieving 2017-01-14 00:00:00...done\n",
      "Retrieving 2017-01-21 00:00:00...done\n",
      "Retrieving 2017-01-28 00:00:00...done\n",
      "Retrieving 2017-02-04 00:00:00...done\n",
      "Retrieving 2017-02-11 00:00:00...done\n",
      "Retrieving 2017-02-18 00:00:00...done\n",
      "Retrieving 2017-02-25 00:00:00...done\n",
      "Retrieving 2017-03-04 00:00:00...done\n",
      "Retrieving 2017-03-11 00:00:00...done\n",
      "Retrieving 2017-03-18 00:00:00...done\n",
      "Retrieving 2017-03-25 00:00:00...done\n",
      "Retrieving 2017-04-01 00:00:00...done\n",
      "Retrieving 2017-04-08 00:00:00...done\n",
      "Retrieving 2017-04-15 00:00:00...done\n",
      "Retrieving 2017-04-22 00:00:00...done\n",
      "Retrieving 2017-04-29 00:00:00...done\n",
      "Retrieving 2017-05-06 00:00:00...done\n",
      "Retrieving 2017-05-13 00:00:00...done\n",
      "Retrieving 2017-05-20 00:00:00...done\n"
     ]
    }
   ],
   "source": [
    "# We'll now iterate through our date range, computing the buy and sell volume for each type on\n",
    "# each day.\n",
    "#\n",
    "side_volume_data = []\n",
    "for next_date in sat_date_range:\n",
    "    order_book = OrderBook.get_data_frame(dates=[next_date], types=targets, regions=[region_id], \n",
    "                                          config=dict(local_storage=\".\", tree=True, skip_missing=True, \n",
    "                                                      fill_gaps=True, verbose=True))\n",
    "    order_book = order_book[order_book.location_id == station_id]\n",
    "    # Compute buy/sell volume for each type\n",
    "    for type_id in targets:\n",
    "        buy_volume, sell_volume = compute_side_volume(type_id, next_date, order_book, volume_thresh_map)\n",
    "        side_volume_data.append(dict(day=next_date, type_id=type_id, buy_volume=buy_volume, sell_volume=sell_volume))\n",
    "#\n",
    "# Finally, we convert the side volume data into a dataframe\n",
    "side_volume_df = pd.DataFrame(side_volume_data, index=[x['day'] for x in side_volume_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type_id\n",
      "4025     20\n",
      "4027     20\n",
      "5971     20\n",
      "10998    20\n",
      "35658    20\n",
      "35659    20\n",
      "Name: day, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we've inferred buy and sell volume, we'll need to determine acceptable thresholds\n",
    "# for considering an asset well balanced on a given day.  In an ideal world, buy and sell volume would\n",
    "# each make up half the day's volume.  This rarely happens, so instead we'll expect at least one side\n",
    "# to carry a reasonable amount of volume.  We'll arbitrarily choose 20%.  That is, if both buy and sell\n",
    "# volume make up at least 20% of the total volume then we'll call the day balanced.  Note, also, that\n",
    "# the actual volume ratio gives us a hint on how much volume we could expect to transact on a given\n",
    "# day (hint: it's the smaller of the two volumes).\n",
    "#\n",
    "# The following code determines which asset types are balanced for each day in the target date range.\n",
    "#\n",
    "balanced_volume_threshold = 0.2\n",
    "side_volume_df_copy = side_volume_df.copy()\n",
    "side_volume_df_copy['total_volume'] = side_volume_df_copy.buy_volume + side_volume_df_copy.sell_volume\n",
    "side_volume_df_copy['buy_ratio'] = side_volume_df_copy.buy_volume / side_volume_df_copy.total_volume\n",
    "side_volume_df_copy['sell_ratio'] = side_volume_df_copy.sell_volume / side_volume_df_copy.total_volume\n",
    "buy_exceeds_threshold = side_volume_df_copy[side_volume_df_copy.buy_ratio > balanced_volume_threshold]\n",
    "both_exceed_threshold = buy_exceeds_threshold[buy_exceeds_threshold.sell_ratio > balanced_volume_threshold]\n",
    "#\n",
    "# We can now view for each asset type how many days meet our requirements\n",
    "#\n",
    "day_vol_counts = both_exceed_threshold.groupby(both_exceed_threshold.type_id).day.count()\n",
    "print(day_vol_counts[day_vol_counts == len(sat_date_range)])\n",
    "#\n",
    "# The index of this result represents our new target set\n",
    "#\n",
    "targets = day_vol_counts[day_vol_counts == len(sat_date_range)].index\n",
    "len(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# It follows that lowering the balanced volume threshold would admit more types for consideration.\n",
    "#\n",
    "# We're now down to our final filter which is to look at competition on these remaining types.  You may recall\n",
    "# from the previous example that we detected competition by looking for order price changes.  We reasoned that\n",
    "# the number of orders which change in a given time period is a reasonable upper bound for the number of\n",
    "# active market participants.  We can use this information either to decide a given asset is too active, or\n",
    "# to help formulate a strategy for dealing with a number of other competitors.\n",
    "#\n",
    "# To create our final filter, we need our order change counter from the previous example.\n",
    "#\n",
    "# The following function counts the number of orders which have changed price in a given\n",
    "# interval.  The result is a Pandas DataFrame indexed by start snapshot time and containing \n",
    "# the columns:\n",
    "#\n",
    "# time - the time when one or more orders changed\n",
    "# type_id - type ID which changed\n",
    "# change_count - the number of orders which changed\n",
    "#\n",
    "def count_order_changes(order_book, type_list, sample_interval, verbose=False):\n",
    "    samples = order_book.resample(sample_interval)\n",
    "    total_samples = len(samples)\n",
    "    changes = []\n",
    "    if verbose:\n",
    "        print(\"Checking %d samples for market participants\" % total_samples, flush=True)    \n",
    "    count = 0\n",
    "    #\n",
    "    for sample_group in samples:\n",
    "        #\n",
    "        # Each group is a pair (sample_time, sample_dataframe)\n",
    "        sample_time = sample_group[0]\n",
    "        sample = sample_group[1]\n",
    "        if verbose:\n",
    "            print(\"X\", end='', flush=True)\n",
    "            count += 1\n",
    "            if count % 72 == 0:\n",
    "                print()\n",
    "        #\n",
    "        # Iterate through each type in the type list\n",
    "        for next_type in type_list:\n",
    "            # Reduce this sample by type\n",
    "            by_type = sample[sample.type_id == next_type]\n",
    "            # Group by orders\n",
    "            orders = by_type.groupby(['order_id'])\n",
    "            # Count the unique prices for each order, flag those orders with more than\n",
    "            # one price in the samnple interval.\n",
    "            changed = orders['price'].nunique() > 1\n",
    "            # Count how many orders changed price at least once in the sample interval.\n",
    "            count = changed[changed == True].count()\n",
    "            # Save the number of orders which changed prices\n",
    "            changes.append(dict(time=sample_time, type_id=next_type, change_count=count))\n",
    "    if verbose:\n",
    "        print(flush=True)\n",
    "    return pd.DataFrame(changes, index=[x['time'] for x in changes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving 2017-01-07 00:00:00...done\n",
      "Retrieving 2017-01-14 00:00:00...done\n",
      "Retrieving 2017-01-21 00:00:00...done\n",
      "Retrieving 2017-01-28 00:00:00...done\n",
      "Retrieving 2017-02-04 00:00:00...done\n",
      "Retrieving 2017-02-11 00:00:00...done\n",
      "Retrieving 2017-02-18 00:00:00...done\n",
      "Retrieving 2017-02-25 00:00:00...done\n",
      "Retrieving 2017-03-04 00:00:00...done\n",
      "Retrieving 2017-03-11 00:00:00...done\n",
      "Retrieving 2017-03-18 00:00:00...done\n",
      "Retrieving 2017-03-25 00:00:00...done\n",
      "Retrieving 2017-04-01 00:00:00...done\n",
      "Retrieving 2017-04-08 00:00:00...done\n",
      "Retrieving 2017-04-15 00:00:00...done\n",
      "Retrieving 2017-04-22 00:00:00...done\n",
      "Retrieving 2017-04-29 00:00:00...done\n",
      "Retrieving 2017-05-06 00:00:00...done\n",
      "Retrieving 2017-05-13 00:00:00...done\n",
      "Retrieving 2017-05-20 00:00:00...done\n"
     ]
    }
   ],
   "source": [
    "# Once again, we'll evaluate our remaining targets against the order book for each day in\n",
    "# our target range.  On each day, we'll accumulate order changes for our target types.\n",
    "# Order changes are accumulated over a sampling interval which should be set according\n",
    "# to how frequently you're willing to update orders.  For this example, we'll set the sampling\n",
    "# interval to 30 minutes.  In other words, we'll plan to refresh our orders every 30 minutes.\n",
    "# This will give us 48 change count samples for each asset type on each day.\n",
    "#\n",
    "change_count_data = []\n",
    "for next_date in sat_date_range:\n",
    "    order_book = OrderBook.get_data_frame(dates=[next_date], types=targets, regions=[region_id], \n",
    "                                          config=dict(local_storage=\".\", tree=True, skip_missing=True, \n",
    "                                                      fill_gaps=True, verbose=True))\n",
    "    order_book = order_book[order_book.location_id == station_id]\n",
    "    # Compute and store change count for this day\n",
    "    change_count_data.append(count_order_changes(order_book, targets, '30min'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average:\n",
      "type_id\n",
      "4025     0.856250\n",
      "4027     0.558333\n",
      "5971     1.020833\n",
      "10998    1.097917\n",
      "35658    0.729167\n",
      "35659    1.341667\n",
      "Name: change_count, dtype: float64\n",
      "\n",
      "Median:\n",
      "type_id\n",
      "4025     0\n",
      "4027     0\n",
      "5971     0\n",
      "10998    0\n",
      "35658    0\n",
      "35659    1\n",
      "Name: change_count, dtype: int64\n",
      "\n",
      "Max:\n",
      "type_id\n",
      "4025     15\n",
      "4027      7\n",
      "5971     16\n",
      "10998    11\n",
      "35658     7\n",
      "35659     8\n",
      "Name: change_count, dtype: int64\n",
      "\n",
      "95%:\n",
      "type_id\n",
      "4025     5.0\n",
      "4027     3.0\n",
      "5971     4.0\n",
      "10998    5.0\n",
      "35658    4.0\n",
      "35659    5.0\n",
      "Name: change_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# We now have change counts for every 30 minutes on every day in the test data range.\n",
    "# The choice of acceptable competition is highly subjective but let's apply a bit more \n",
    "# knowledge of our trading pattern to try to create more useful results.\n",
    "#\n",
    "# First, let's collect all change counts into a single dataframe\n",
    "#\n",
    "all_changes = change_count_data[0].append(change_count_data[1:])\n",
    "#\n",
    "# We're filtering for trading on a Saturday, so let's assume trading hours from 1200 UTC\n",
    "# to 2400 UTC.  This is just after down time to midnight EVE time.  In reality, it's likely\n",
    "# we'll trade past midnight UTC, but do perform that analysis we'd need two order books for\n",
    "# each day.  So for now, we'll go with these hours.\n",
    "#\n",
    "# Let's eliminate all data outside of these hours.\n",
    "#\n",
    "constrained = all_changes[all_changes.index.hour >= 12]\n",
    "#\n",
    "# What's left can now be used to measure change count behavior.  But how should we do this?\n",
    "# There are many possibilities:\n",
    "#\n",
    "# 1. Average change count\n",
    "# 2. Median change count\n",
    "# 3. Max change count\n",
    "# 4. Some other quantile of change count\n",
    "#\n",
    "# Average or median change count will give us some expectation of what a typical time period\n",
    "# might look like in terms of competition, but we risk the danger of undershooting the number\n",
    "# of orders we need to maintain if the average or mean is unusually low.  Conversely, a\n",
    "# measure of max count shows how bad things could get.  If we wanted to be extremely conservative,\n",
    "# we could use max as a guideline for competition.  Finally, we could use a quantile, say the 95%\n",
    "# change count quantile.  This would tell us, for example, the max change count for 95% of the\n",
    "# sample intervals.\n",
    "#\n",
    "# Let's look at all of these measures before we make a decision.\n",
    "#\n",
    "print(\"Average:\")\n",
    "print(constrained.groupby(constrained.type_id).change_count.mean())\n",
    "print(\"\\nMedian:\")\n",
    "print(constrained.groupby(constrained.type_id).change_count.median())\n",
    "print(\"\\nMax:\")\n",
    "print(constrained.groupby(constrained.type_id).change_count.max())\n",
    "print(\"\\n95%:\")\n",
    "print(constrained.groupby(constrained.type_id).change_count.quantile(0.95))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average and median numbers look very promising while the max numbers show where we might see extreme competition.  The 95% measure shows again that these types are all somewhat similar in terms of likely competition.\n",
    "\n",
    "At this point, we're left with a very subjective decision.  For the sake of completing this example, let's choose to accept any type in which there are 5 or fewer changes at the 95% interval.  This means that we know have our final set of types on which we'll attempt to make a market.\n",
    "\n",
    "Further analysis is possible using the techniques described in the previous example.  For example, we could graph change data to see what times of day are most active for these types.  We'll leave such further analsysis to the reader, although we will investigate more careful analysis later in the chapter where we discuss trading simulation.\n",
    "\n",
    "For the record, these are the names of the final six types our filter produced:\n",
    "\n",
    "* X5 Enduring Stasis Webifier\n",
    "* Fleeting Compact Stasis Webifier\n",
    "* 5MN Cold-Gas Enduring Microwarpdrive\n",
    "* Warp Core Stabilizer I\n",
    "* 5MN Quad LiF Restrained Microwarpdrive\n",
    "* 50MN Y-T8 Compact Microwarpdrive"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
