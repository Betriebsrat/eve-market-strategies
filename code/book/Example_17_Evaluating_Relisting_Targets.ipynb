{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 17 - Evaluating Relisting Targets\n",
    "\n",
    "In this example, we'll attempt to find good targets for a relisting strategy as described in the text.  We'll evaluate relisting targets according to the following criteria:\n",
    "\n",
    "1. **High Buy Volume** - There should be ample buy volume in the market.  Our main risk with this strategy is that we are unable to sell our orders before enough competition arrives to undercut us below profitability.  As long as buy volume is strong, this risk is low.\n",
    "2. **Low Sell Volume** - There should *not* be too much sell volume in the market.  A market with both strong buy and sell volume will make it too easy for competitors to buy and sell competitively.  In this type of market, relisting only helps our competitors by pushing up ask prices even further.  We'll have competition and it will be difficult to sell.\n",
    "3. **Low Sell Side Competition** - There shouldn't be too much competition on the sell side.  The more competition, the more we'll need to push up sell prices to be profitable.\n",
    "4. **Cost** - It should not be too expensive to buy out the ask side of the market.  Our strategy will need to set a limit for what we are willing to pay.  Note that we may not need to buy out the entire ask side, just enough to allow the placement of sell orders at profit.\n",
    "\n",
    "Once we've selected likely relisting targets, we'll consider the likely success of a strategy like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame, Series\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "# EveKit imports`\n",
    "from evekit.reference import Client\n",
    "from evekit.util import convert_raw_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using region_id=10000002, station_id=60003760 from 2017-01-07 00:00:00 to 2017-05-20 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# As in previous examples, we'll consider opportunities in The Forge at the busiest station in Jita.\n",
    "# We'll look for market making targets which are consistently strong over an approximately five\n",
    "# month historical period.  It's likely such assets will continue to be good relisting targets\n",
    "# in the near future but, of course, historical performance does not guarantee future returns.\n",
    "#\n",
    "sde_client = Client.SDE.get()\n",
    "region_query = \"{values: ['The Forge']}\"\n",
    "station_query = \"{values: ['Jita IV - Moon 4 - Caldari Navy Assembly Plant']}\"\n",
    "region_id = sde_client.Map.getRegions(regionName=region_query).result()[0][0]['regionID']\n",
    "station_id = sde_client.Station.getStations(stationName=station_query).result()[0][0]['stationID']\n",
    "date_range = pd.date_range(datetime.datetime(2017, 1, 7), datetime.datetime(2017, 5, 20))\n",
    "print(\"Using region_id=%d, station_id=%d from %s to %s\" % (region_id, station_id, str(date_range[0]), str(date_range[-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11782"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll filter for general liquidity first, starting from all available types.  This will eliminate\n",
    "# assets with obvious flaws.\n",
    "#\n",
    "market_types = Client.SDE.load_complete(sde_client.Inventory.getTypes, marketGroupID=\"{start: 0, end: 1000000000}\")\n",
    "market_type_map = {}\n",
    "for x in market_types:\n",
    "    market_type_map[x['typeID']] = x\n",
    "len(market_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving 2017-01-07 00:00:00...done\n",
      "Retrieving 2017-01-14 00:00:00...done\n",
      "Retrieving 2017-01-21 00:00:00...done\n",
      "Retrieving 2017-01-28 00:00:00...done\n",
      "Retrieving 2017-02-04 00:00:00...done\n",
      "Retrieving 2017-02-11 00:00:00...done\n",
      "Retrieving 2017-02-18 00:00:00...done\n",
      "Retrieving 2017-02-25 00:00:00...done\n",
      "Retrieving 2017-03-04 00:00:00...done\n",
      "Retrieving 2017-03-11 00:00:00...done\n",
      "Retrieving 2017-03-18 00:00:00...done\n",
      "Retrieving 2017-03-25 00:00:00...done\n",
      "Retrieving 2017-04-01 00:00:00...done\n",
      "Retrieving 2017-04-08 00:00:00...done\n",
      "Retrieving 2017-04-15 00:00:00...done\n",
      "Retrieving 2017-04-22 00:00:00...done\n",
      "Retrieving 2017-04-29 00:00:00...done\n",
      "Retrieving 2017-05-06 00:00:00...done\n",
      "Retrieving 2017-05-13 00:00:00...done\n",
      "Retrieving 2017-05-20 00:00:00...done\n"
     ]
    }
   ],
   "source": [
    "# As we've done with other market making targets, for this example we'll limit ourselves to \n",
    "# Saturdays only.  Thus we'll only load market history for this restricted date range.\n",
    "#\n",
    "from evekit.marketdata import MarketHistory\n",
    "sat_date_range = [x for x in date_range if x.weekday() == 5]\n",
    "market_history = MarketHistory.get_data_frame(dates=sat_date_range, types=market_type_map.keys(), regions=[region_id], \n",
    "                                              config=dict(local_storage=\".\", tree=True, skip_missing=True, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Include the liquidity filter framework from previous examples.\n",
    "#\n",
    "def liquid_types(history, liquidp, verbose=False):\n",
    "    # Result is a map from region to set of liquid types for that region\n",
    "    # Iterate through all types contained in the history object\n",
    "    liquid_map = {}\n",
    "    count = 0\n",
    "    # Iterate through all regions and types\n",
    "    for next_region in history.region_id.unique():\n",
    "        liquid_set = set()\n",
    "        by_region = history[history.region_id == next_region]\n",
    "        for next_type in by_region.type_id.unique():\n",
    "            by_type = by_region[by_region.type_id == next_type]\n",
    "            if liquidp(next_region, next_type, by_type):\n",
    "                liquid_set.add(next_type)\n",
    "            count += 1\n",
    "            if count % 1000 == 0 and verbose:\n",
    "                print(\"Tested %d (region, type) pairs\" % count)\n",
    "        liquid_map[next_region] = liquid_set\n",
    "    return liquid_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In this example, we'll filter for assets which trade every day in our historic range, \n",
    "# with a reasonable number of orders.  Unlike the pure market making strategy, we're ignoring\n",
    "# the price averaged volume since we'll be adding more detailed checks later to determine\n",
    "# exact trade volume.  If it turns out that good candidates always meet a certain price\n",
    "# averaged volume test, then we can add this criteria back in.\n",
    "#\n",
    "def liquidity_filter(min_days, min_count):\n",
    "    def liquidp(region_id, type_id, history):\n",
    "        return len(history) >= min_days and \\\n",
    "               len(history[history.order_count < min_count]) == 0\n",
    "    return liquidp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We'll use the following values to parameterize the liquidity filter.\n",
    "#\n",
    "# Minimum number of orders per day\n",
    "min_count = 250\n",
    "# Each type we consider must trade every day\n",
    "min_values = len(market_history.index.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now compute liquid types.  The liquidity filter returns a map from region to the\n",
    "# set of liquid types in that region.\n",
    "#\n",
    "liquid_type_map = liquid_types(market_history, liquidity_filter(min_values, min_count))\n",
    "len(liquid_type_map[region_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving 2016-12-10 00:00:00...done\n",
      "Retrieving 2016-12-17 00:00:00...done\n",
      "Retrieving 2016-12-24 00:00:00...done\n",
      "Retrieving 2016-12-31 00:00:00...done\n",
      "Retrieving 2017-01-07 00:00:00...done\n",
      "Retrieving 2017-01-14 00:00:00...done\n",
      "Retrieving 2017-01-21 00:00:00...done\n",
      "Retrieving 2017-01-28 00:00:00...done\n",
      "Retrieving 2017-02-04 00:00:00...done\n",
      "Retrieving 2017-02-11 00:00:00...done\n",
      "Retrieving 2017-02-18 00:00:00...done\n",
      "Retrieving 2017-02-25 00:00:00...done\n",
      "Retrieving 2017-03-04 00:00:00...done\n",
      "Retrieving 2017-03-11 00:00:00...done\n",
      "Retrieving 2017-03-18 00:00:00...done\n",
      "Retrieving 2017-03-25 00:00:00...done\n",
      "Retrieving 2017-04-01 00:00:00...done\n",
      "Retrieving 2017-04-08 00:00:00...done\n",
      "Retrieving 2017-04-15 00:00:00...done\n",
      "Retrieving 2017-04-22 00:00:00...done\n",
      "Retrieving 2017-04-29 00:00:00...done\n",
      "Retrieving 2017-05-06 00:00:00...done\n",
      "Retrieving 2017-05-13 00:00:00...done\n",
      "Retrieving 2017-05-20 00:00:00...done\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a reduced set of reasonably liquid types, we'll move to our first two filters\n",
    "# which compare buy side and sell side volume.  This means we'll need to extract trades from order books\n",
    "# which means we'll need our trade extraction code from previous examples.  That code starts by \n",
    "# calculating a volume threshold which helps us distinguish large trades from cancel orders.  Since\n",
    "# that calculation uses a window'd average, we need to load a bit more market history which we\n",
    "# do here.\n",
    "#\n",
    "target_types = liquid_type_map[region_id]\n",
    "ext_date_range = [datetime.datetime(2016, 12, 10), datetime.datetime(2016, 12, 17),\n",
    "                  datetime.datetime(2016, 12, 24), datetime.datetime(2016, 12, 31)] + sat_date_range\n",
    "ext_market_history = MarketHistory.get_data_frame(dates=ext_date_range, types=target_types, regions=[region_id], \n",
    "                                                  config=dict(local_storage=\".\", tree=True, skip_missing=True, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we can compute the thresholds we need for trade inferrence\n",
    "#\n",
    "volume_thresh_multiplier = 0.04\n",
    "volume_thresh_map = {}\n",
    "for next_type in ext_market_history.groupby(ext_market_history.type_id):\n",
    "    group_id = next_type[0]\n",
    "    group_df = next_type[1]\n",
    "    volume_thresh_map[group_id] = group_df.volume.rolling(window=5, center=False).mean() * volume_thresh_multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is our side volume computer from an earlier example.  It will extract trades and compute the total\n",
    "# buy and sell side trade volume for a given type and day.\n",
    "#\n",
    "def compute_side_volume(type_id, dt, order_book, volume_threshold_map):\n",
    "    buy_volume = 0\n",
    "    sell_volume = 0\n",
    "    by_type = order_book[order_book.type_id == type_id]\n",
    "    vol_limit = volume_threshold_map[type_id][dt]\n",
    "    #\n",
    "    # Iterate over consecutive book snapshots looking for order book changes.\n",
    "    #\n",
    "    snap_list = list(by_type.groupby(by_type.index))\n",
    "    snap_pairs = zip(snap_list, snap_list[1:])\n",
    "    for current, next in snap_pairs:\n",
    "        current_snap = current[1]\n",
    "        current_time = current[0]\n",
    "        next_snap = next[1]\n",
    "        next_time = next[0]\n",
    "        # Look for volume changes.  These are trades.\n",
    "        merged = pd.merge(current_snap, next_snap, on=\"order_id\")\n",
    "        changed_orders = merged[merged.volume_x != merged.volume_y]\n",
    "        for next_change in changed_orders.index:\n",
    "            # Create the trade object\n",
    "            next_line = changed_orders.ix[next_change]\n",
    "            amount = next_line.volume_x - next_line.volume_y \n",
    "            if next_line.buy_x:\n",
    "                buy_volume += amount\n",
    "            else:\n",
    "                sell_volume += amount\n",
    "        # Look for removed orders.  These are either a fully filled order or a cancel.\n",
    "        removed_orders = set(current_snap.order_id).difference(set(next_snap.order_id))\n",
    "        current_order_list = list(current_snap.order_id)\n",
    "        for order_id in removed_orders:\n",
    "            order_count = current_order_list.count(order_id)\n",
    "            next_line = current_snap[current_snap.order_id == order_id].ix[current_time]\n",
    "            # If the volume of a removed order does not exceed the threshold, then it's a trade\n",
    "            if order_count > 1:\n",
    "                # Handle duplicate orders found in some data\n",
    "                next_line = next_line.iloc[0]\n",
    "            if next_line.volume <= vol_limit:\n",
    "                if next_line.buy:\n",
    "                    buy_volume += next_line.volume\n",
    "                else:\n",
    "                    sell_volume += next_line.volume\n",
    "    # Return result\n",
    "    return (buy_volume, sell_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving 2017-01-07 00:00:00...done\n",
      "Retrieving 2017-01-14 00:00:00...done\n",
      "Retrieving 2017-01-21 00:00:00...done\n",
      "Retrieving 2017-01-28 00:00:00...done\n",
      "Retrieving 2017-02-04 00:00:00...done\n",
      "Retrieving 2017-02-11 00:00:00...done\n",
      "Retrieving 2017-02-18 00:00:00...done\n",
      "Retrieving 2017-02-25 00:00:00...done\n",
      "Retrieving 2017-03-04 00:00:00...done\n",
      "Retrieving 2017-03-11 00:00:00...done\n",
      "Retrieving 2017-03-18 00:00:00...done\n",
      "Retrieving 2017-03-25 00:00:00...done\n",
      "Retrieving 2017-04-01 00:00:00...done\n",
      "Retrieving 2017-04-08 00:00:00...done\n",
      "Retrieving 2017-04-15 00:00:00...done\n",
      "Retrieving 2017-04-22 00:00:00...done\n",
      "Retrieving 2017-04-29 00:00:00...done\n",
      "Retrieving 2017-05-06 00:00:00...done\n",
      "Retrieving 2017-05-13 00:00:00...done\n",
      "Retrieving 2017-05-20 00:00:00...done\n"
     ]
    }
   ],
   "source": [
    "# This code iterates through each order book in our date range and computes the buy and sell\n",
    "# volume for each type on each day.  We'll assemble these values into a DataFrame for\n",
    "# further analysis.\n",
    "#\n",
    "from evekit.marketdata import OrderBook\n",
    "\n",
    "side_volume_data = []\n",
    "for next_date in sat_date_range:\n",
    "    order_book = OrderBook.get_data_frame(dates=[next_date], types=target_types, regions=[region_id], \n",
    "                                          config=dict(local_storage=\".\", tree=True, skip_missing=True, \n",
    "                                                      fill_gaps=True, verbose=True))\n",
    "    order_book = order_book[order_book.location_id == station_id]\n",
    "    # Compute buy/sell volume for each type\n",
    "    for type_id in target_types:\n",
    "        buy_volume, sell_volume = compute_side_volume(type_id, next_date, order_book, volume_thresh_map)\n",
    "        side_volume_data.append(dict(day=next_date, type_id=type_id, buy_volume=buy_volume, sell_volume=sell_volume))\n",
    "#\n",
    "# Finally, we convert the side volume data into a dataframe\n",
    "side_volume_df = pd.DataFrame(side_volume_data, index=[x['day'] for x in side_volume_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type_id\n",
      "4405     17\n",
      "11370    17\n",
      "11578    17\n",
      "21640    16\n",
      "22291    16\n",
      "26929    18\n",
      "28668    18\n",
      "31213    19\n",
      "31358    16\n",
      "31360    18\n",
      "31372    17\n",
      "31716    16\n",
      "31788    19\n",
      "31790    19\n",
      "31796    19\n",
      "40519    20\n",
      "Name: day, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We're now ready to apply our first two filters which require:\n",
    "#\n",
    "# 1. High buy side volume, we want mostly buyers in our market, which implies:\n",
    "# 2. Low sell side volume, we want few sellers in our market.\n",
    "#\n",
    "# Without loss of generality, we'll impose a threshold on buy volume.  For this example, we'll use\n",
    "# 80%.  That is, we'll keep a target type if trades against ask orders make up at least 80% of\n",
    "# total volume.  It will likely be rare for this to happen every day in our date range, so we'll\n",
    "# only require this threshold to be met for 75% of our target dates.\n",
    "#\n",
    "# Lowering the volume threshold, or lowering the required day count will admit more types,\n",
    "# but we argue these will be less successful relisting targets.\n",
    "#\n",
    "sell_volume_threshold = 0.80\n",
    "sell_day_count_threshold = 0.75\n",
    "side_volume_df_copy = side_volume_df.copy()\n",
    "side_volume_df_copy['total_volume'] = side_volume_df_copy.buy_volume + side_volume_df_copy.sell_volume\n",
    "side_volume_df_copy['buy_ratio'] = side_volume_df_copy.buy_volume / side_volume_df_copy.total_volume\n",
    "side_volume_df_copy['sell_ratio'] = side_volume_df_copy.sell_volume / side_volume_df_copy.total_volume\n",
    "sell_exceeds_threshold = side_volume_df_copy[side_volume_df_copy.sell_ratio > sell_volume_threshold]\n",
    "#\n",
    "# We can now view for each asset type how many days meet our requirements\n",
    "#\n",
    "day_vol_counts = sell_exceeds_threshold.groupby(sell_exceeds_threshold.type_id).day.count()\n",
    "print(day_vol_counts[day_vol_counts > len(sat_date_range) * sell_day_count_threshold])\n",
    "#\n",
    "# The index of this result represents our new target set\n",
    "#\n",
    "targets = day_vol_counts[day_vol_counts > len(sat_date_range) * sell_day_count_threshold].index\n",
    "len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our next filter requires measuring competition on our types which we defined earlier as the number of\n",
    "# existing orders which change price in a given time interval.  More competitors will make it more likely\n",
    "# that someone will undercut our sell orders right after we buy out the ask side for relisting.\n",
    "#\n",
    "# We'll pull in our previous code for extracting order changes, except that we'll make a few small\n",
    "# changes so that we only count sell order changes.  This is where we'll experience any competition.\n",
    "#\n",
    "def count_sell_order_changes(order_book, type_list, sample_interval, verbose=False):\n",
    "    by_side = order_book[order_book.buy == False]\n",
    "    samples = by_side.resample(sample_interval)\n",
    "    total_samples = len(samples)\n",
    "    changes = []\n",
    "    if verbose:\n",
    "        print(\"Checking %d samples for market participants\" % total_samples, flush=True)    \n",
    "    count = 0\n",
    "    #\n",
    "    for sample_group in samples:\n",
    "        #\n",
    "        # Each group is a pair (sample_time, sample_dataframe)\n",
    "        sample_time = sample_group[0]\n",
    "        sample = sample_group[1]\n",
    "        if verbose:\n",
    "            print(\"X\", end='', flush=True)\n",
    "            count += 1\n",
    "            if count % 72 == 0:\n",
    "                print()\n",
    "        #\n",
    "        # Iterate through each type in the type list\n",
    "        for next_type in type_list:\n",
    "            # Reduce this sample by type\n",
    "            by_type = sample[sample.type_id == next_type]\n",
    "            # Group by orders\n",
    "            orders = by_type.groupby(['order_id'])\n",
    "            # Count the unique prices for each order, flag those orders with more than\n",
    "            # one price in the samnple interval.\n",
    "            changed = orders['price'].nunique() > 1\n",
    "            # Count how many orders changed price at least once in the sample interval.\n",
    "            count = changed[changed == True].count()\n",
    "            # Save the number of orders which changed prices\n",
    "            changes.append(dict(time=sample_time, type_id=next_type, change_count=count))\n",
    "    if verbose:\n",
    "        print(flush=True)\n",
    "    return pd.DataFrame(changes, index=[x['time'] for x in changes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving 2017-01-07 00:00:00...done\n",
      "Retrieving 2017-01-14 00:00:00...done\n",
      "Retrieving 2017-01-21 00:00:00...done\n",
      "Retrieving 2017-01-28 00:00:00...done\n",
      "Retrieving 2017-02-04 00:00:00...done\n",
      "Retrieving 2017-02-11 00:00:00...done\n",
      "Retrieving 2017-02-18 00:00:00...done\n",
      "Retrieving 2017-02-25 00:00:00...done\n",
      "Retrieving 2017-03-04 00:00:00...done\n",
      "Retrieving 2017-03-11 00:00:00...done\n",
      "Retrieving 2017-03-18 00:00:00...done\n",
      "Retrieving 2017-03-25 00:00:00...done\n",
      "Retrieving 2017-04-01 00:00:00...done\n",
      "Retrieving 2017-04-08 00:00:00...done\n",
      "Retrieving 2017-04-15 00:00:00...done\n",
      "Retrieving 2017-04-22 00:00:00...done\n",
      "Retrieving 2017-04-29 00:00:00...done\n",
      "Retrieving 2017-05-06 00:00:00...done\n",
      "Retrieving 2017-05-13 00:00:00...done\n",
      "Retrieving 2017-05-20 00:00:00...done\n"
     ]
    }
   ],
   "source": [
    "# Now we'll loop through every day in our date range and extract competition information.\n",
    "# We'll use 30 minute intervals for his filter.  You can use a smaller interval if you plan\n",
    "# to monitor your orders more frequently.\n",
    "#\n",
    "change_count_data = []\n",
    "for next_date in sat_date_range:\n",
    "    order_book = OrderBook.get_data_frame(dates=[next_date], types=targets, regions=[region_id], \n",
    "                                          config=dict(local_storage=\".\", tree=True, skip_missing=True, \n",
    "                                                      fill_gaps=True, verbose=True))\n",
    "    order_book = order_book[order_book.location_id == station_id]\n",
    "    # Compute and store change count for this day\n",
    "    change_count_data.append(count_sell_order_changes(order_book, targets, '30min'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>95%</th>\n",
       "      <th>Average</th>\n",
       "      <th>Max</th>\n",
       "      <th>Median</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4405</th>\n",
       "      <td>4.00</td>\n",
       "      <td>1.129167</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11370</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.397917</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11578</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.397917</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21640</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0.622917</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22291</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0.712500</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26929</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0.477083</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28668</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0.581250</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31213</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31358</th>\n",
       "      <td>4.00</td>\n",
       "      <td>1.147917</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31360</th>\n",
       "      <td>5.00</td>\n",
       "      <td>1.489583</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31372</th>\n",
       "      <td>5.05</td>\n",
       "      <td>1.277083</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31716</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0.554167</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31788</th>\n",
       "      <td>4.00</td>\n",
       "      <td>0.943750</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31790</th>\n",
       "      <td>6.00</td>\n",
       "      <td>1.422917</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31796</th>\n",
       "      <td>6.00</td>\n",
       "      <td>1.910417</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40519</th>\n",
       "      <td>8.00</td>\n",
       "      <td>2.577083</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          95%   Average  Max  Median\n",
       "type_id                             \n",
       "4405     4.00  1.129167    7       1\n",
       "11370    2.00  0.397917    5       0\n",
       "11578    2.00  0.397917    3       0\n",
       "21640    3.00  0.622917    5       0\n",
       "22291    3.00  0.712500    8       0\n",
       "26929    3.00  0.477083    7       0\n",
       "28668    3.00  0.581250    5       0\n",
       "31213    2.00  0.406250    5       0\n",
       "31358    4.00  1.147917    9       0\n",
       "31360    5.00  1.489583   11       1\n",
       "31372    5.05  1.277083   10       0\n",
       "31716    3.00  0.554167    6       0\n",
       "31788    4.00  0.943750   10       0\n",
       "31790    6.00  1.422917   12       1\n",
       "31796    6.00  1.910417   13       1\n",
       "40519    8.00  2.577083   20       2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have competition data, we need to determine what we'll consider as an \n",
    "# acceptable level of competition.  To help decide, we'll present four measures of\n",
    "# competition as we've done in previous examples:\n",
    "#\n",
    "# 1. Average change count\n",
    "# 2. Median change count\n",
    "# 3. Max change count\n",
    "# 4. Some other quantile of change count\n",
    "#\n",
    "# We'll also apply an optimization we applied before, which is to assume we'll only be monitoring\n",
    "# our orders between 1200 UTC to 2400 UTC.\n",
    "#\n",
    "all_changes = change_count_data[0].append(change_count_data[1:])\n",
    "constrained = all_changes[all_changes.index.hour >= 12]\n",
    "pd.DataFrame({ 'Average': constrained.groupby(constrained.type_id).change_count.mean(),\n",
    "               'Median' : constrained.groupby(constrained.type_id).change_count.median(),\n",
    "               'Max': constrained.groupby(constrained.type_id).change_count.max(),\n",
    "               '95%': constrained.groupby(constrained.type_id).change_count.quantile(0.95)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11370 Prototype Cloaking Device I\n",
      "11578 Covert Ops Cloaking Device II\n",
      "21640 Valkyrie II\n",
      "22291 Ballistic Control System II\n",
      "26929 Small Processor Overclocking Unit I\n",
      "28668 Nanite Repair Paste\n",
      "31213 Small Gravity Capacitor Upgrade I\n",
      "31716 Small Anti-EM Screen Reinforcer I\n"
     ]
    }
   ],
   "source": [
    "# What is acceptable competition?  This is highly subjective but obviously increasing your\n",
    "# threshold will admit more types.  On average, all types look acceptable but the maximum\n",
    "# values show areas of higher competition.  As in previous examples, we like using the 95%\n",
    "# quantile as it helps reveal whether the maximum is a true outlier, or closer to actual behavior.\n",
    "#\n",
    "# For this example, we'll set our 95% quantile threshold at 4 meaning we'll accept any type\n",
    "# with a 95% quantile order change count less than four.  Let's see what types this threshold\n",
    "# will allow.\n",
    "#\n",
    "less_threshold = 4\n",
    "less_count = constrained.groupby(constrained.type_id).change_count.quantile(0.95) < less_threshold\n",
    "for x in less_count[less_count == True].index:\n",
    "    print(str(x) + \" \" + market_type_map[x]['typeName'])\n",
    "low_comp_targets = list(less_count[less_count == True].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From these last eight types, we're now ready to apply our last filter which will compute\n",
    "# the buyout cost we need to incur in order to relist and profit at a given return\n",
    "# target.\n",
    "#\n",
    "# This filter computes all transaction costs, so we'll define tax rate and broker rate here.\n",
    "#\n",
    "tax_rate = 0.01\n",
    "broker_rate = 0.025\n",
    "#\n",
    "# Target return is the return on cost we intend to realize from relisting.  Lowering this\n",
    "# value we reduce buyout cost, but of course also reduces profit.\n",
    "#\n",
    "target_return = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To compute buyout, we'll accumulate buyout orders in each snapshot until we've accumulated enough\n",
    "# orders so that we can sell at a price high enough to meet our return target.  We also require\n",
    "# that there are no sell orders below our sell price in the order book.  Such orders would sell\n",
    "# before us, and likely require that we reprice (which would not meet our return target).\n",
    "#\n",
    "# The function below computes the buyout amount for a single type on an order book representing\n",
    "# a snapshot.\n",
    "#\n",
    "def compute_buyout(type_id, target_return, order_book, tax_rate, broker_rate):\n",
    "    # Compute proceeds from sale less tax and broker fees\n",
    "    def compute_net(amount, price):\n",
    "        return (amount * price) - (amount * price * tax_rate) - (amount * price * broker_rate)\n",
    "    # Compute minimum sell price based on current cost\n",
    "    def compute_sell_price(amount, cost):\n",
    "        if amount == 0:\n",
    "            return None\n",
    "        return (cost * (target_return + 1))/(amount * (1 - tax_rate - broker_rate))\n",
    "    # Check whether there are orders below our required sell price.\n",
    "    def check_target_met(order_list, cost, count):\n",
    "        if len(order_list) == 0:\n",
    "            return True\n",
    "        price = compute_sell_price(count, cost)\n",
    "        if price is None:\n",
    "            return False\n",
    "        return order_list[0]['price'] >= price\n",
    "    # Now work through the book buying up the buy side until we can sell at a price\n",
    "    # which meets our return target.\n",
    "    asset_count = 0\n",
    "    asset_cost = 0                                                \n",
    "    by_type = order_book[order_book.type_id == type_id]\n",
    "    by_side = by_type[by_type.buy == False]\n",
    "    # Copy orders into consumable objects\n",
    "    order_list = []\n",
    "    for next_order in by_side.iterrows():\n",
    "        order_list.append(dict(price=next_order[1]['price'], volume=next_order[1]['volume']))\n",
    "    # Now consume until we meet our return target\n",
    "    while not check_target_met(order_list, asset_cost, asset_count):\n",
    "        asset_count += order_list[0]['volume']\n",
    "        asset_cost += order_list[0]['price'] * order_list[0]['volume']\n",
    "        order_list.pop(0)\n",
    "    sell_price = compute_sell_price(asset_count, asset_cost)\n",
    "    gross_gain = -1 if sell_price is None else compute_net(asset_count, sell_price)\n",
    "    net_gain = -1 if gross_gain == -1 else gross_gain - asset_cost\n",
    "    return (asset_count, asset_cost, sell_price, gross_gain, net_gain)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following function computes buyout amounts from each snapshot for each type\n",
    "# across the order book for a single day.  The result is returned as a DataFrame.\n",
    "#\n",
    "def collect_all_buyouts(order_book, targets, tax_rate, broker_rate, target_return):\n",
    "    #\n",
    "    # Iterate over book snapshots\n",
    "    #\n",
    "    buyout_results = []\n",
    "    snap_list = list(order_book.groupby(order_book.index))\n",
    "    total = len(targets)\n",
    "    print(\"[\" + ('-'*total) + \"]\")\n",
    "    print(\"[\", end='')\n",
    "    for next_target in targets:\n",
    "        print('+', end='')\n",
    "        for next_group in snap_list:\n",
    "            timestamp = next_group[0]\n",
    "            snapshot = next_group[1]\n",
    "            buyout_data = compute_buyout(next_target, target_return, snapshot, tax_rate, broker_rate)\n",
    "            buyout_results.append(dict(time=timestamp, type_id=next_target, count=buyout_data[0], \n",
    "                                       price=buyout_data[2], cost=buyout_data[1], profit=buyout_data[4]))\n",
    "    print(']')\n",
    "    return pd.DataFrame(buyout_results, index=[x['time'] for x in buyout_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving 2017-01-07 00:00:00...done\n",
      "Retrieving 2017-01-14 00:00:00...done\n",
      "Retrieving 2017-01-21 00:00:00...done\n",
      "Retrieving 2017-01-28 00:00:00...done\n",
      "Retrieving 2017-02-04 00:00:00...done\n",
      "Retrieving 2017-02-11 00:00:00...done\n",
      "Retrieving 2017-02-18 00:00:00...done\n",
      "Retrieving 2017-02-25 00:00:00...done\n",
      "Retrieving 2017-03-04 00:00:00...done\n",
      "Retrieving 2017-03-11 00:00:00...done\n",
      "Retrieving 2017-03-18 00:00:00...done\n",
      "Retrieving 2017-03-25 00:00:00...done\n",
      "Retrieving 2017-04-01 00:00:00...done\n",
      "Retrieving 2017-04-08 00:00:00...done\n",
      "Retrieving 2017-04-15 00:00:00...done\n",
      "Retrieving 2017-04-22 00:00:00...done\n",
      "Retrieving 2017-04-29 00:00:00...done\n",
      "Retrieving 2017-05-06 00:00:00...done\n",
      "Retrieving 2017-05-13 00:00:00...done\n",
      "Retrieving 2017-05-20 00:00:00...done\n",
      "[--------]\n",
      "[++++++++]\n"
     ]
    }
   ],
   "source": [
    "# Now we'll collect buyout data across all snapshots and all dates in our date\n",
    "# range.  Because we are down to only a few types, we can request the entire order book\n",
    "# at once.\n",
    "#\n",
    "order_book = OrderBook.get_data_frame(dates=sat_date_range, types=low_comp_targets, regions=[region_id], \n",
    "                                      config=dict(local_storage=\".\", tree=True, skip_missing=True, \n",
    "                                                  fill_gaps=True, verbose=True))\n",
    "order_book = order_book[order_book.location_id == station_id]\n",
    "buyout_data = collect_all_buyouts(order_book, low_comp_targets, tax_rate, broker_rate, target_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type_id\n",
       "31213       10,963,416.79\n",
       "31716       11,429,221.53\n",
       "26929       15,021,073.09\n",
       "22291      375,556,540.29\n",
       "11370      501,940,916.74\n",
       "21640      537,484,436.48\n",
       "11578      607,184,377.00\n",
       "28668    1,598,871,006.66\n",
       "Name: profit, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The output data frame has the following columms:\n",
    "#\n",
    "# cost - the cost of buying out enough of the sell orders for each snapshot\n",
    "# count - the number of units purchased to buy out a snapshot\n",
    "# price - the price for the sell order we would need to place with our purchased assets to meet our return target\n",
    "# profit - the profit we would expect to make if we sold all assets less sales tax and broker fees\n",
    "#\n",
    "# We've set our return target to guarantee we make at least 5% on our investment.  Let's look first\n",
    "# at how this translates into profit.  We'll display the average profit across all snapshots for each\n",
    "# type.\n",
    "#\n",
    "isk_format = lambda x: \"{0:,.2f}\".format(x)\n",
    "buyout_data.groupby(buyout_data.type_id)['profit'].mean().sort_values().apply(isk_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type_id\n",
       "31213       219,268,335.85\n",
       "31716       228,584,430.60\n",
       "26929       300,421,461.89\n",
       "22291     7,511,130,805.74\n",
       "11370    10,038,818,334.73\n",
       "21640    10,749,688,729.58\n",
       "11578    12,143,687,540.09\n",
       "28668    31,977,420,133.13\n",
       "Name: cost, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some types report very high profit, but since we know these values represent a 5% return, \n",
    "# the corresponding cost of these investments must be large.\n",
    "#\n",
    "buyout_data.groupby(buyout_data.type_id)['cost'].mean().sort_values().apply(isk_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type_id\n",
       "11578        2,376.15\n",
       "26929        3,269.02\n",
       "31213        3,572.03\n",
       "31716        4,485.74\n",
       "11370        6,101.84\n",
       "22291        7,702.98\n",
       "21640       10,609.14\n",
       "28668    1,077,253.43\n",
       "Name: count, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before we decide whether some or all of these opportunities are worth our time, it's important\n",
    "# to look at the average unit count as compared to daily volume.  The average unit count represents\n",
    "# the number of units we need to purchase to buy out a side, and therefore the number of units\n",
    "# we'll need to sell after relisting.  Here are the results for this example:\n",
    "#\n",
    "buyout_data.groupby(buyout_data.type_id)['count'].mean().sort_values().apply(isk_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type_id\n",
       "11370      1215.05\n",
       "11578      1284.55\n",
       "21640      2473.30\n",
       "22291      3659.60\n",
       "26929       997.75\n",
       "28668    529547.20\n",
       "31213      1043.15\n",
       "31716      1489.05\n",
       "Name: sell_volume, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You may recall from above that we've already computed sell volume (that is, volume of trades buying\n",
    "# from sell orders).  We can extract and average this data to get an idea how our counts compare to\n",
    "# average sell volume.\n",
    "#\n",
    "side_volume_df_copy.groupby(side_volume_df_copy.type_id).sell_volume.mean()[buyout_data.type_id.unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving 2017-01-07 00:00:00...done\n",
      "Retrieving 2017-01-14 00:00:00...done\n",
      "Retrieving 2017-01-21 00:00:00...done\n",
      "Retrieving 2017-01-28 00:00:00...done\n",
      "Retrieving 2017-02-04 00:00:00...done\n",
      "Retrieving 2017-02-11 00:00:00...done\n",
      "Retrieving 2017-02-18 00:00:00...done\n",
      "Retrieving 2017-02-25 00:00:00...done\n",
      "Retrieving 2017-03-04 00:00:00...done\n",
      "Retrieving 2017-03-11 00:00:00...done\n",
      "Retrieving 2017-03-18 00:00:00...done\n",
      "Retrieving 2017-03-25 00:00:00...done\n",
      "Retrieving 2017-04-01 00:00:00...done\n",
      "Retrieving 2017-04-08 00:00:00...done\n",
      "Retrieving 2017-04-15 00:00:00...done\n",
      "Retrieving 2017-04-22 00:00:00...done\n",
      "Retrieving 2017-04-29 00:00:00...done\n",
      "Retrieving 2017-05-06 00:00:00...done\n",
      "Retrieving 2017-05-13 00:00:00...done\n",
      "Retrieving 2017-05-20 00:00:00...done\n",
      "[--------]\n",
      "[++++++++]\n"
     ]
    }
   ],
   "source": [
    "# If we look at type 11578 as an example, we see that we'll need to sell about twice the average daily\n",
    "# volume in order to sell all relisted assets.  Most of the other assets have a similar relationship\n",
    "# (or worse).\n",
    "#\n",
    "# This analysis suggests that perhaps 5% is too aggressive of a return target.  Let's lower our target to\n",
    "# 2% to see if there is any improvement.\n",
    "#\n",
    "order_book = OrderBook.get_data_frame(dates=sat_date_range, types=low_comp_targets, regions=[region_id], \n",
    "                                      config=dict(local_storage=\".\", tree=True, skip_missing=True, \n",
    "                                                  fill_gaps=True, verbose=True))\n",
    "order_book = order_book[order_book.location_id == station_id]\n",
    "tp_buyout_data = collect_all_buyouts(order_book, low_comp_targets, tax_rate, broker_rate, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type_id\n",
      "31213       173,392,965.48\n",
      "31716       224,912,762.15\n",
      "26929       278,003,279.98\n",
      "22291     6,620,699,134.93\n",
      "11578     9,365,789,668.19\n",
      "11370     9,760,760,562.63\n",
      "21640    10,178,280,648.71\n",
      "28668    27,288,382,029.60\n",
      "Name: cost, dtype: object\n",
      "type_id\n",
      "11578      1,856.33\n",
      "31213      2,901.74\n",
      "26929      3,035.23\n",
      "31716      4,434.19\n",
      "11370      5,956.31\n",
      "22291      6,840.73\n",
      "21640     10,076.57\n",
      "28668    924,684.12\n",
      "Name: count, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Cost and count should both be lower, which is the case as shown below.\n",
    "#\n",
    "print(tp_buyout_data.groupby(buyout_data.type_id)['cost'].mean().sort_values().apply(isk_format))\n",
    "print(tp_buyout_data.groupby(buyout_data.type_id)['count'].mean().sort_values().apply(isk_format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type_id\n",
       "11370      1215.05\n",
       "11578      1284.55\n",
       "21640      2473.30\n",
       "22291      3659.60\n",
       "26929       997.75\n",
       "28668    529547.20\n",
       "31213      1043.15\n",
       "31716      1489.05\n",
       "Name: sell_volume, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we compare counts to average sell volume again, we see that some types are a little more reasonable\n",
    "# but it's still the case that every time will have to have an above average day.  This implies that\n",
    "# any relisting strategy would need to extend into the next day.\n",
    "#\n",
    "side_volume_df_copy.groupby(side_volume_df_copy.type_id).sell_volume.mean()[tp_buyout_data.type_id.unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type_id\n",
       "31213      3,467,859.31\n",
       "31716      4,498,255.24\n",
       "26929      5,560,065.60\n",
       "22291    132,413,982.70\n",
       "11578    187,315,793.36\n",
       "11370    195,215,211.25\n",
       "21640    203,565,612.97\n",
       "28668    545,767,640.59\n",
       "Name: profit, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, for completeness, let's look at the profit for this case.\n",
    "#\n",
    "tp_buyout_data.groupby(buyout_data.type_id)['profit'].mean().sort_values().apply(isk_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost benefit for the relisting strategy doesn't seem very promising, at least for the region and thresholds we provided.  It is possible that this strategy works better in other environments with less overall competition.  We leave this analysis to the reader."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
